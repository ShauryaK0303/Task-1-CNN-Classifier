{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShauryaK0303/Task_3_CNN_Classifier/blob/ann/ANN_LAB_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUES10:**\n",
        "\n",
        "# **(a) Write a program to implement gradient ascent and descent algorithm.**\n",
        "\n",
        "# **(b) Write a program to implement Newton Method.**"
      ],
      "metadata": {
        "id": "_xO7xj0tzulQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8j0RKp8zVPr",
        "outputId": "eaa32bb4-5c4a-47f5-e3ef-6b5799de1fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANIRUDH SHUKLA 00619011921 AIDS B1 A\n"
          ]
        }
      ],
      "source": [
        "print(\"ANIRUDH SHUKLA 00619011921 AIDS B1 A\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRADIENT ASCENT AND DESCENT ALGORITHM"
      ],
      "metadata": {
        "id": "gaonwjY10CTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zdQxDyt5w0Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Ascent Algortihm\n",
        "def gradient_ascent_all_points(function, gradient, starting_point, learning_rate, iterations):\n",
        "  point = starting_point\n",
        "  all_ascent_points = [point.copy()]\n",
        "  for _ in range(iterations):\n",
        "    ascent = [+ learning_rate * grad for grad in gradient(point)]\n",
        "    point = [p + d for p, d in zip(point, ascent)]\n",
        "    all_ascent_points.append(point.copy())\n",
        "  return all_ascent_points"
      ],
      "metadata": {
        "id": "N_oPnFpHzsRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent Algortihm\n",
        "def gradient_descent_all_points(function, gradient, starting_point, learning_rate, iterations):\n",
        "  point = starting_point\n",
        "  all_descent_points = [point.copy()]\n",
        "  for _ in range(iterations):\n",
        "    descent = [- learning_rate * grad for grad in gradient(point)]\n",
        "    point = [p + d for p, d in zip(point, descent)]\n",
        "    all_descent_points.append(point.copy())\n",
        "  return all_descent_points"
      ],
      "metadata": {
        "id": "mR8Z09T8LQpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x,y):\n",
        "  return (2 * x * y) + (2 * x) - (x ** 2) - (2 * (y ** 2))\n",
        "\n",
        "# f_x(x,y) = (2*y)+2-(2*x) Gradient of the function with respect to x\n",
        "# f_y(x,y) = (2*x)-(4*y) Gradient of the function with respect to y\n",
        "\n",
        "def f_gradient(point):\n",
        "  x, y = point\n",
        "  return [(2 * y) + 2 - (2 * x) , (2 * x) - (4 * y)]"
      ],
      "metadata": {
        "id": "9fb_szSxMEGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1\n",
        "iterations = 10\n",
        "starting_point = [2, -2]"
      ],
      "metadata": {
        "id": "MU970LadLps4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All ascent points:\")\n",
        "count = 0\n",
        "all_ascent_points = gradient_ascent_all_points(f, f_gradient, starting_point, learning_rate, iterations)\n",
        "for point in all_ascent_points:\n",
        "  count += 1\n",
        "  print(f\"At iteration {count}:\",point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BMA7hqbbzHM",
        "outputId": "4f4bc00a-9af0-45ad-affc-7c4005724ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All ascent points:\n",
            "At iteration 1: [2, -2]\n",
            "At iteration 2: [-4, 10]\n",
            "At iteration 3: [26, -38]\n",
            "At iteration 4: [-100, 166]\n",
            "At iteration 5: [434, -698]\n",
            "At iteration 6: [-1828, 2962]\n",
            "At iteration 7: [7754, -12542]\n",
            "At iteration 8: [-32836, 53134]\n",
            "At iteration 9: [139106, -225074]\n",
            "At iteration 10: [-589252, 953434]\n",
            "At iteration 11: [2496122, -4038806]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All descent points:\")\n",
        "count = 0\n",
        "all_descent_points = gradient_descent_all_points(f, f_gradient, starting_point, learning_rate, iterations)\n",
        "for point in all_descent_points:\n",
        "  count += 1\n",
        "  print(f\"At iteration {count}:\",point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTgNDZd2brJ9",
        "outputId": "7dd639f4-9439-4279-f482-df423c2f9350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All descent points:\n",
            "At iteration 1: [2, -2]\n",
            "At iteration 2: [8, -14]\n",
            "At iteration 3: [50, -86]\n",
            "At iteration 4: [320, -530]\n",
            "At iteration 5: [2018, -3290]\n",
            "At iteration 6: [12632, -20486]\n",
            "At iteration 7: [78866, -127694]\n",
            "At iteration 8: [491984, -796202]\n",
            "At iteration 9: [3068354, -4964978]\n",
            "At iteration 10: [19135016, -30961598]\n",
            "At iteration 11: [119328242, -193078022]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEWTON METHOD"
      ],
      "metadata": {
        "id": "7KLEKXZ1z657"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x,y):\n",
        "  return (x * y) + (4 * y) - (3 * (x ** 2)) - (y**2)\n",
        "\n",
        "# f_x(x,y) = (y)-(6*x) Gradient of the function with respect to x\n",
        "# f_y(x,y) = (x)+4-(2*y) Gradient of the function with respect to y\n",
        "# f_xx(x,y) = -6 Gradient of the function with respect to xx\n",
        "# f_xy(x,y) = 1 Gradient of the function with respect to xy\n",
        "# f_yx(x,y) = 1 Gradient of the function with respect to yx\n",
        "# f_yy(x,y) = -2 Gradient of the function with respect to yy\n",
        "\n",
        "def f_gradient(point):\n",
        "  x, y = point\n",
        "  return [(y) - (6 * x), (x) + 4 - (2 * y)]\n",
        "\n",
        "def f_hessian(point):\n",
        "  x, y = point\n",
        "  return [[-6, 1], [1, -2]]"
      ],
      "metadata": {
        "id": "RTz4ot3S7JyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def newton_method(function, gradient, hessian, starting_point, iterations):\n",
        "  point = starting_point\n",
        "  all_points = [point.copy()]\n",
        "  for _ in range(iterations):\n",
        "    g = gradient(point)\n",
        "    H = hessian(point)\n",
        "    try:\n",
        "      inverse_H = np.linalg.inv(H)\n",
        "    except np.linalg.LinAlgError:\n",
        "      print(\"Hessian matrix is singular or poorly conditioned. Convergence not guaranteed.\")\n",
        "      break\n",
        "    newton_step = - np.dot(inverse_H, g)\n",
        "    point = point + newton_step\n",
        "    all_points.append(point.copy())\n",
        "  return all_points"
      ],
      "metadata": {
        "id": "ghr3IRVtozlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starting_point = [1, -1]\n",
        "iterations = 10"
      ],
      "metadata": {
        "id": "-ppkfiRcrWFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"All Newton points:\")\n",
        "count = 0\n",
        "all_newton_points = newton_method(f, f_gradient, f_hessian, starting_point, iterations)\n",
        "for point in all_newton_points:\n",
        "  count += 1\n",
        "  print(f\"At iteration {count}:\",point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZie_MTBrdgF",
        "outputId": "e4bf1efa-d431-4614-c0f9-3da6469cb07b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Newton points:\n",
            "At iteration 1: [1, -1]\n",
            "At iteration 2: [0.36363636 2.18181818]\n",
            "At iteration 3: [0.36363636 2.18181818]\n",
            "At iteration 4: [0.36363636 2.18181818]\n",
            "At iteration 5: [0.36363636 2.18181818]\n",
            "At iteration 6: [0.36363636 2.18181818]\n",
            "At iteration 7: [0.36363636 2.18181818]\n",
            "At iteration 8: [0.36363636 2.18181818]\n",
            "At iteration 9: [0.36363636 2.18181818]\n",
            "At iteration 10: [0.36363636 2.18181818]\n",
            "At iteration 11: [0.36363636 2.18181818]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "**Gradient Descent:**\n",
        "\n",
        "Gradient descent converges to a local minimum of the function. It iteratively updates its position in the direction of the steepest descent, meaning it finds a point where the function's value is lower than its immediate neighbors. However, it may not find the global minimum (the absolute lowest point) if there are multiple valleys in the function's landscape.\n",
        "\n",
        "**Gradient Ascent:**\n",
        "\n",
        "Gradient ascent converges to a local maximum of the function. It works similarly to gradient descent, but uses the negative gradient to move towards the steepest ascent, ultimately reaching a point where the function's value is higher than its immediate neighbors. This method does not guarantee finding the global maximum (the absolute highest point) either.\n",
        "\n",
        "**Newton's Method:**\n",
        "\n",
        "Newton's method can converge to a local minimum, maximum, or saddle point of the function, depending on the initial guess and the curvature of the function at that point. It uses the gradient and the Hessian (matrix of second-order derivatives) to take larger steps in the direction of the optimum. However, it requires careful consideration as:\n",
        "It may not converge if the starting guess is far from the optimum.\n",
        "It may diverge if the Hessian is singular or poorly conditioned.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "Direction: Gradient descent moves towards lower function values, while gradient ascent moves towards higher values. Newton's method can move in either direction depending on the curvature of the function.\n",
        "\n",
        "Guarantees: Gradient descent and ascent only guarantee finding local extrema (minimum/maximum). Newton's method may not even converge in some cases.\n",
        "\n",
        "Speed: Newton's method can converge faster than gradient descent methods, especially when the initial guess is close to the optimum.\n",
        "\n",
        "In conclusion, these methods provide different approaches to finding extrema of functions. Gradient descent/ascent offer simplicity and reliability, while Newton's method offers potential speed benefits but requires more caution in its application. Understanding these characteristics is crucial for selecting the most suitable method for your optimization tasks."
      ],
      "metadata": {
        "id": "oPn6kWfj0bQr"
      }
    }
  ]
}